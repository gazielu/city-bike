{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25eab6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.wwo_hist_spark import retrieve_city_data,retrieve_hist_data\n",
    "from utils.files import AwsStorage\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "from prefect import flow, task\n",
    "from prefect_aws.s3 import S3Bucket\n",
    "import pandas as pd\n",
    "from prefect_aws import AwsCredentials\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e536daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('utils/city-bike.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "AWS_KEY_ID = os.getenv('AWS_KEY_ID')\n",
    "AWS_SECRET = os.getenv('AWS_SECRET')\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "STORAGE_BUCKET_NAME = os.getenv('STORAGE_BUCKET_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14768d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 10:38:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/03 10:38:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark S3 Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1f7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_object = AwsStorage(AWS_REGION,AWS_KEY_ID,AWS_SECRET,STORAGE_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabb05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def fetch() -> pd.DataFrame:\n",
    "    \"\"\"Read taxi data from web into pandas DataFrame\"\"\"\n",
    "    # if randint(0, 1) > 0:\n",
    "    #     raise Exception\n",
    "\n",
    "    \n",
    "\n",
    "    frequency=3\n",
    "    year = 2018\n",
    "    minth =9\n",
    "    start_date,end_date = get_month_dates(year, month)#'01-DEC-2018'\n",
    "    #end_date = get_month_dates(2018, 12)#'30-DEC-2018'\n",
    "    api_key = '25c4af770ad84987be7181434232203'\n",
    "    location_list = ['new-york']\n",
    "\n",
    "    hist_weather_data = retrieve_hist_data(api_key,\n",
    "                                    location_list,\n",
    "                                    start_date,\n",
    "                                    end_date,\n",
    "                                    frequency,\n",
    "                                    location_label = False,\n",
    "                                    export_csv = True,\n",
    "                                    store_df = True)\n",
    "\n",
    "    \n",
    "    return hist_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700f070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "def get_month_dates(year, month):\n",
    "    # Determine the number of days in the given month\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    \n",
    "    # Create date objects for the first and last days of the month\n",
    "    start_date = datetime(year, month, 1)\n",
    "    end_date = datetime(year, month, num_days)\n",
    "    \n",
    "    # Format the date strings as \"DD-MMM-YYYY\"\n",
    "    start_date_str = start_date.strftime(\"%d-%b-%Y\")\n",
    "    end_date_str = end_date.strftime(\"%d-%b-%Y\")\n",
    "    \n",
    "    return start_date_str, end_date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2a518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('01-Apr-2023', '30-Apr-2023')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> get_month_dates(2023, 4)\n",
    "('01-Apr-2023', '30-Apr-2023')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc46ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieving weather data for new-york\n",
      "\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/tslibs/conversion.pyx\", line 530, in pandas._libs.tslibs.conversion._convert_str_to_tsobject\n",
      "  File \"pandas/_libs/tslibs/parsing.pyx\", line 320, in pandas._libs.tslibs.parsing.parse_datetime_string\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\", line 1368, in parse\n",
      "    return DEFAULTPARSER.parse(timestr, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\", line 643, in parse\n",
      "    raise ParserError(\"Unknown string format: %s\", timestr)\n",
      "dateutil.parser._parser.ParserError: Unknown string format: 30-DEV-2018\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_26983/790873801.py\", line 7, in <module>\n",
      "    hist_weather_data = retrieve_hist_data(api_key,\n",
      "  File \"/opt/workspace/src/utils/wwo_hist_spark.py\", line 116, in retrieve_hist_data\n",
      "    df_this_city = retrieve_this_location(api_key, location, start_date, end_date, frequency, response_cache_path)\n",
      "  File \"/opt/workspace/src/utils/wwo_hist_spark.py\", line 66, in retrieve_this_location\n",
      "    list_mon_begin = pd.date_range(start_date, end_date, freq='MS', closed='right')\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/datetimes.py\", line 1125, in date_range\n",
      "    dtarr = DatetimeArray._generate_range(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/datetimes.py\", line 364, in _generate_range\n",
      "    end = Timestamp(end)\n",
      "  File \"pandas/_libs/tslibs/timestamps.pyx\", line 1698, in pandas._libs.tslibs.timestamps.Timestamp.__new__\n",
      "  File \"pandas/_libs/tslibs/conversion.pyx\", line 249, in pandas._libs.tslibs.conversion.convert_to_tsobject\n",
      "  File \"pandas/_libs/tslibs/conversion.pyx\", line 533, in pandas._libs.tslibs.conversion._convert_str_to_tsobject\n",
      "ValueError: could not convert string to Timestamp\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1049, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1003, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1129, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 958, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "    frequency=3\n",
    "    start_date = '01-DEC-2018'\n",
    "    end_date = '30-DEV-2018'\n",
    "    api_key = '25c4af770ad84987be7181434232203'\n",
    "    location_list = ['new-york']\n",
    "\n",
    "    hist_weather_data = retrieve_hist_data(api_key,\n",
    "                                    location_list,\n",
    "                                    start_date,\n",
    "                                    end_date,\n",
    "                                    frequency,\n",
    "                                    location_label = False,\n",
    "                                    export_csv = True,\n",
    "                                    store_df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1804e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f32d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieving weather data for new-york\n",
      "\n",
      "\n",
      "Currently retrieving data for new-york: from 2018-11-01 to 2018-11-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29818/1343480355.py:69: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_mon_begin = pd.date_range(start_date, end_date, freq='MS', closed='right')\n",
      "/tmp/ipykernel_29818/1343480355.py:74: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_mon_end = pd.date_range(start_date, end_date, freq='M', closed='left')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:00:01.184430\n",
      "\n",
      "\n",
      "export new-york completed!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = fetch()\n",
    "df  = fetch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "sparkDF=spark.createDataFrame(pandasDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deef82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# assuming the original list is named \"original_list\"\n",
    "reshaped_array = np.array(df).reshape(488, 25)\n",
    "dfs = pd.DataFrame(reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95c6036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>01:44 AM</td>\n",
       "      <td>02:40 PM</td>\n",
       "      <td>08:01 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>347</td>\n",
       "      <td>9</td>\n",
       "      <td>new-york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-01 03:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>01:44 AM</td>\n",
       "      <td>02:40 PM</td>\n",
       "      <td>08:01 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>new-york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-01 06:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>01:44 AM</td>\n",
       "      <td>02:40 PM</td>\n",
       "      <td>08:01 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1023</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>new-york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-01 09:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>01:44 AM</td>\n",
       "      <td>02:40 PM</td>\n",
       "      <td>08:01 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1023</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>new-york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-01 12:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>01:44 AM</td>\n",
       "      <td>02:40 PM</td>\n",
       "      <td>08:01 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1023</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>new-york</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0  1  2    3    4  5   6         7         8         9   \\\n",
       "0 2018-12-01 00:00:00  7  1  0.0  7.0  2  36  01:44 AM  02:40 PM  08:01 AM   \n",
       "1 2018-12-01 03:00:00  7  1  0.0  7.0  2  36  01:44 AM  02:40 PM  08:01 AM   \n",
       "2 2018-12-01 06:00:00  7  1  0.0  7.0  2  36  01:44 AM  02:40 PM  08:01 AM   \n",
       "3 2018-12-01 09:00:00  7  1  0.0  7.0  2  36  01:44 AM  02:40 PM  08:01 AM   \n",
       "4 2018-12-01 12:00:00  7  1  0.0  7.0  2  36  01:44 AM  02:40 PM  08:01 AM   \n",
       "\n",
       "   ...  15  16  17   18    19 20  21   22 23        24  \n",
       "0  ...  10  10  75  0.0  1020  3  10  347  9  new-york  \n",
       "1  ...  10   4  78  0.0  1021  3  10    4  9  new-york  \n",
       "2  ...   5  10  78  0.0  1023  2  10    6  5  new-york  \n",
       "3  ...   9  50  69  0.0  1023  4  10   63  8  new-york  \n",
       "4  ...  13  59  65  0.0  1023  6  10  100  8  new-york  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3abbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39947d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Install s3fs to access S3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/registry.py:212\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     register_implementation(protocol, _import_class(bit[\u001b[39m\"\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/registry.py:235\u001b[0m, in \u001b[0;36m_import_class\u001b[0;34m(cls, minv)\u001b[0m\n\u001b[1;32m    234\u001b[0m mod, name \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mrsplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod)\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(mod, name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 's3fs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m sdf \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(pdf)\n\u001b[1;32m     13\u001b[0m \u001b[39m# write the PySpark DataFrame to a Parquet file on S3\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pdf\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39ms3://databricks-ug/city-bike/Landing/test_file\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m \u001b[39m#sdf.write.mode('overwrite').csv('s3://databricks-ug/city-bike/Landing/test_file')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3721\u001b[0m     path_or_buf,\n\u001b[1;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3737\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    714\u001b[0m     path_or_buf,\n\u001b[1;32m    715\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    716\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    717\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    718\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    719\u001b[0m )\n\u001b[1;32m    721\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:409\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     file_obj \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39;49mopen(\n\u001b[1;32m    410\u001b[0m         filepath_or_buffer, mode\u001b[39m=\u001b[39;49mfsspec_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(storage_options \u001b[39mor\u001b[39;49;00m {})\n\u001b[1;32m    411\u001b[0m     )\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    412\u001b[0m \u001b[39m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/core.py:419\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(urlpath, mode, compression, encoding, errors, protocol, newline, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    370\u001b[0m     urlpath,\n\u001b[1;32m    371\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    378\u001b[0m ):\n\u001b[1;32m    379\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a path or paths, return one ``OpenFile`` object.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m    ``OpenFile`` object.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m open_files(\n\u001b[1;32m    420\u001b[0m         urlpath\u001b[39m=\u001b[39;49m[urlpath],\n\u001b[1;32m    421\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    422\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    423\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    424\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    425\u001b[0m         protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m    426\u001b[0m         newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    427\u001b[0m         expand\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    428\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    429\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/core.py:272\u001b[0m, in \u001b[0;36mopen_files\u001b[0;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_files\u001b[39m(\n\u001b[1;32m    204\u001b[0m     urlpath,\n\u001b[1;32m    205\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m ):\n\u001b[1;32m    217\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a path or paths, return a list of ``OpenFile`` objects.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[39m    For writing, a str path must contain the \"*\" character, which will be filled\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m    be used as a single context\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     fs, fs_token, paths \u001b[39m=\u001b[39m get_fs_token_paths(\n\u001b[1;32m    273\u001b[0m         urlpath,\n\u001b[1;32m    274\u001b[0m         mode,\n\u001b[1;32m    275\u001b[0m         num\u001b[39m=\u001b[39;49mnum,\n\u001b[1;32m    276\u001b[0m         name_function\u001b[39m=\u001b[39;49mname_function,\n\u001b[1;32m    277\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m    278\u001b[0m         protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m    279\u001b[0m         expand\u001b[39m=\u001b[39;49mexpand,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m auto_mkdir:\n\u001b[1;32m    282\u001b[0m         parents \u001b[39m=\u001b[39m {fs\u001b[39m.\u001b[39m_parent(path) \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/core.py:574\u001b[0m, in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39mif\u001b[39;00m protocol:\n\u001b[1;32m    573\u001b[0m     storage_options[\u001b[39m\"\u001b[39m\u001b[39mprotocol\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m protocol\n\u001b[0;32m--> 574\u001b[0m chain \u001b[39m=\u001b[39m _un_chain(urlpath0, storage_options \u001b[39mor\u001b[39;49;00m {})\n\u001b[1;32m    575\u001b[0m inkwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    576\u001b[0m \u001b[39m# Reverse iterate the chain, creating a nested target_* structure\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/core.py:315\u001b[0m, in \u001b[0;36m_un_chain\u001b[0;34m(path, kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mfor\u001b[39;00m bit \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(bits):\n\u001b[1;32m    314\u001b[0m     protocol \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mprotocol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m split_protocol(bit)[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m get_filesystem_class(protocol)\n\u001b[1;32m    316\u001b[0m     extra_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_kwargs_from_urls(bit)\n\u001b[1;32m    317\u001b[0m     kws \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(protocol, {})\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/registry.py:214\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    212\u001b[0m         register_implementation(protocol, _import_class(bit[\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m    213\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(bit[\u001b[39m\"\u001b[39m\u001b[39merr\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m registry[protocol]\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotocol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: Install s3fs to access S3"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a Pandas DataFrame\n",
    "pdf = pd.DataFrame({'col1': [1, 2, 3], 'col2': ['a', 'b', 'c']})\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName('pandas-to-spark').getOrCreate()\n",
    "\n",
    "# convert the Pandas DataFrame to a PySpark DataFrame\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "# write the PySpark DataFrame to a Parquet file on S3\n",
    "pdf.to_csv('s3://databricks-ug/city-bike/Landing/test_file')\n",
    "#sdf.write.mode('overwrite').csv('s3://databricks-ug/city-bike/Landing/test_file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae501882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use the pyspark library to write a Spark DataFrame as a Parquet file and then use the boto3 library to upload the file to S3. Here's an example code snippet:\n",
    "\n",
    "\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "#spark = SparkSession.builder.appName(\"Write to S3\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\"), (3, \"baz\")], [\"id\", \"value\"])\n",
    "\n",
    "# Write DataFrame to Parquet file\n",
    "df.write.parquet(\"example\")\n",
    "\n",
    "\n",
    "#bucket_name = \"databricks-ug\"\n",
    "#directory_name = \"city-bike\" #it's name of your folders\n",
    "\n",
    "# Upload Parquet file to S3 using boto3\n",
    "#s3 = boto3.client(\"s3\")\n",
    "#s3.upload_file(\"example.parquet\", \"databricks-ug\", \"city-bike/Landing/example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3dffb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy_folder_to_s3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m copy_folder_to_s3(\u001b[39m'\u001b[39m\u001b[39mexample/\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatabricks-ug\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcity-bike/Landaing\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy_folder_to_s3' is not defined"
     ]
    }
   ],
   "source": [
    " copy_folder_to_s3('example/','databricks-ug','city-bike/Landaing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ed5a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload: example/part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/_SUCCESS to target: city-bike/Landaing/example/_SUCCESS ...Success\n",
      "Upload: example/._SUCCESS.crc to target: city-bike/Landaing/example/._SUCCESS.crc ...Success\n",
      "Upload: example/part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/.part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "client = boto3.client('s3', \n",
    "                          region_name=AWS_REGION,#AWS self._aws_region_name, \n",
    "                          aws_access_key_id=AWS_KEY_ID,# self.__aws_key_id, \n",
    "                          aws_secret_access_key=AWS_SECRET)# self.__aws_secret_access_key)\n",
    "# bucket_name = \"databricks-ug\"\n",
    "# directory_name = \"city-bike\" #it's name of your folders\n",
    "# s3.put_object(Bucket=bucket_name, Key=(directory_name+'/'))\n",
    "\n",
    "\n",
    "# client = boto3.client('s3')\n",
    "local_path = \"example/\"\n",
    "bucketname = \"databricks-ug\"\n",
    "\n",
    "for path, dirs, files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        s3_path = 'city-bike/Landaing'\n",
    "        file_s3 = os.path.normpath(s3_path + '/' + local_path +  file)\n",
    "        file_local = os.path.join(path, file)\n",
    "        print(\"Upload:\", file_local, \"to target:\", file_s3, end=\"\")\n",
    "        client.upload_file(file_local, bucketname, file_s3)\n",
    "        print(\" ...Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3961357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload: example/part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00010-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/_SUCCESS to target: city-bike/Landaing/example/_SUCCESS ...Success\n",
      "Upload: example/._SUCCESS.crc to target: city-bike/Landaing/example/._SUCCESS.crc ...Success\n",
      "Upload: example/part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet to target: city-bike/Landaing/example/part-00005-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet ...Success\n",
      "Upload: example/.part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00015-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n",
      "Upload: example/.part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc to target: city-bike/Landaing/example/.part-00000-c6433db7-5c8c-4bdf-bf57-7f2f8cd9eb3f-c000.snappy.parquet.crc ...Success\n"
     ]
    }
   ],
   "source": [
    " s3_object.copy_folder_to_s3('example/','databricks-ug','city-bike/Landaing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prefect\n",
    "from prefect import task\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "@task\n",
    "def pandas_to_spark_to_s3(pdf_path, parquet_path):\n",
    "    # read the Pandas DataFrame from a file\n",
    "    pdf = pd.read_csv(pdf_path)\n",
    "    \n",
    "    # create a SparkSession\n",
    "    spark = SparkSession.builder.appName('pandas-to-spark').getOrCreate()\n",
    "\n",
    "    # convert the Pandas DataFrame to a PySpark DataFrame\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "    # write the PySpark DataFrame to a Parquet file on S3\n",
    "    sdf.write.mode('overwrite').parquet(parquet_path)\n",
    "    \n",
    "    return parquet_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e41bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow, Parameter\n",
    "\n",
    "with Flow('my-flow') as flow:\n",
    "    pdf_path = Parameter('pdf_path')\n",
    "    parquet_path = Parameter('parquet_path')\n",
    "    \n",
    "    result = pandas_to_spark_to_s3(pdf_path, parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d40739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script is used to retrieve and transform weather data into single csv.\n",
    "example API explorer: https://www.worldweatheronline.com/developer/premium-api-explorer.aspx\n",
    "\n",
    "input: api_key, location_list, start_date, end_date, frequency\n",
    "\n",
    "output: location_name.csv'\n",
    "\n",
    "@author: Ekapope Viriyakovithya\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "##################################\n",
    "# function to unnest json for each month\n",
    "def extract_monthly_data(data):\n",
    "    num_days = len(data)\n",
    "    # initialize df_month to store return data\n",
    "    df_month = pd.DataFrame()\n",
    "    for i in range(num_days):\n",
    "        # extract this day\n",
    "        d = data[i]\n",
    "        # astronomy data is the same for the whole day\n",
    "        astr_df = pd.DataFrame(d['astronomy'])\n",
    "        # hourly data; temperature for each hour of the day\n",
    "        hourly_df = pd.DataFrame(d['hourly'])\n",
    "        # this wanted_key will be duplicated and use 'ffill' to fill up the NAs\n",
    "        wanted_keys = ['date', 'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour', 'uvIndex']  # The keys you want\n",
    "        subset_d = dict((k, d[k]) for k in wanted_keys if k in d)\n",
    "        this_df = pd.DataFrame(subset_d, index=[0])\n",
    "        df = pd.concat([this_df.reset_index(drop=True), astr_df], axis=1)\n",
    "        # concat selected astonomy columns with hourly data\n",
    "        df = pd.concat([df, hourly_df], axis=1)\n",
    "        df = df.fillna(method='ffill')\n",
    "        # make date_time columm to proper format\n",
    "        # fill leading zero for hours to 4 digits (0000-2400 hr)\n",
    "        df['time'] = df['time'].apply(lambda x: x.zfill(4))\n",
    "        # keep only first 2 digit (00-24 hr) \n",
    "        df['time'] = df['time'].str[:2]\n",
    "        # convert to pandas datetime\n",
    "        df['date_time'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "        # keep only interested columns\n",
    "        col_to_keep = ['date_time', 'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour', 'uvIndex',\n",
    "                       'moon_illumination', 'moonrise', 'moonset', 'sunrise', 'sunset',\n",
    "                       'DewPointC', 'FeelsLikeC', 'HeatIndexC', 'WindChillC', 'WindGustKmph',\n",
    "                       'cloudcover', 'humidity', 'precipMM', 'pressure', 'tempC', 'visibility',\n",
    "                       'winddirDegree', 'windspeedKmph']\n",
    "        df = df[col_to_keep]\n",
    "        df = df.loc[:,~df.columns.duplicated()]\n",
    "        df_month = pd.concat([df_month, df])\n",
    "    return (df_month)\n",
    "\n",
    "\n",
    "##################################\n",
    "# function to retrive data by date range and location\n",
    "# default frequency = 1 hr\n",
    "# each month costs 1 request (free trial 500 requests/key, as of 30-May-2019)\n",
    "def retrieve_this_location(api_key, location, start_date, end_date, frequency, response_cache_path):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # create list of first day of month for range between start and end dates non-inclusive (open)\n",
    "    list_mon_begin = pd.date_range(start_date, end_date, freq='MS', closed='right')\n",
    "    # convert to Series and add start_date at beginning\n",
    "    list_mon_begin = pd.concat([pd.Series(pd.to_datetime(start_date)), pd.Series(list_mon_begin)], ignore_index=True)\n",
    "\n",
    "    # create list of month end dates for range between start and end dates non-inclusive (open)\n",
    "    list_mon_end = pd.date_range(start_date, end_date, freq='M', closed='left')\n",
    "    # convert to Series and add end_date at end\n",
    "    list_mon_end = pd.concat([pd.Series(list_mon_end), pd.Series(pd.to_datetime(end_date))], ignore_index=True)\n",
    "\n",
    "    # count number of months to be retrieved\n",
    "    total_months = len(list_mon_begin)\n",
    "\n",
    "    # initialize df_hist to store return data\n",
    "    df_hist = pd.DataFrame()\n",
    "    for m in range(total_months):\n",
    "        start_d = str(list_mon_begin[m])[:10]\n",
    "        end_d = str(list_mon_end[m])[:10]\n",
    "        file_path = f'{response_cache_path}/{location}_{start_d}_{end_d}'\n",
    "        if response_cache_path and os.path.exists(file_path):\n",
    "            print('Reading cached data for ' + location + ': from ' + start_d + ' to ' + end_d)\n",
    "            with open(f'{response_cache_path}/{location}_{start_d}_{end_d}', 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "        else:\n",
    "            print('Currently retrieving data for ' + location + ': from ' + start_d + ' to ' + end_d)\n",
    "            url_page = 'http://api.worldweatheronline.com/premium/v1/past-weather.ashx?key=' + api_key + '&q=' + location + '&format=json&date=' + start_d + '&enddate=' + end_d + '&tp=' + str(\n",
    "                frequency)\n",
    "            json_page = urllib.request.urlopen(url_page, timeout=10)\n",
    "            json_data = json.loads(json_page.read().decode())\n",
    "\n",
    "        if response_cache_path:\n",
    "            with open(f'{response_cache_path}/{location}_{start_d}_{end_d}', 'w') as f:\n",
    "                json.dump(json_data, f)\n",
    "        data = json_data['data']['weather']\n",
    "        # call function to extract json object\n",
    "        df_this_month = extract_monthly_data(data)\n",
    "        df_this_month['location'] = location\n",
    "        df_hist = pd.concat([df_hist, df_this_month])\n",
    "\n",
    "        time_elapsed = datetime.now() - start_time\n",
    "        print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "    return (df_hist)\n",
    "\n",
    "\n",
    "##################################\n",
    "# main function to retrive the data by location list\n",
    "def retrieve_hist_data(api_key, location_list, start_date, end_date, frequency, location_label=False, export_csv=True,\n",
    "                       store_df=False, response_cache_path=None):\n",
    "    result_list = []\n",
    "    for location in location_list:\n",
    "        print('\\n\\nRetrieving weather data for ' + location + '\\n\\n')\n",
    "        df_this_city = retrieve_this_location(api_key, location, start_date, end_date, frequency, response_cache_path)\n",
    "\n",
    "        if (location_label == True):\n",
    "            # add city name as prefix to the colnames\n",
    "            df_this_city = df_this_city.add_prefix(location + '_')\n",
    "            df_this_city.columns.values[0] = 'date_time'\n",
    "\n",
    "        if (export_csv == True):\n",
    "            df_this_city.to_csv('./' + location + '.csv', header=True, index=False)\n",
    "            print('\\n\\nexport ' + location + ' completed!\\n\\n')\n",
    "\n",
    "        if (store_df == True):\n",
    "            # save result as object in the work space\n",
    "            result_list.append(df_this_city)\n",
    "\n",
    "    return (result_list)\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6300224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieving weather data for new-york\n",
      "\n",
      "\n",
      "Currently retrieving data for new-york: from 2018-12-01 to 2018-12-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29818/1343480355.py:69: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_mon_begin = pd.date_range(start_date, end_date, freq='MS', closed='right')\n",
      "/tmp/ipykernel_29818/1343480355.py:74: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_mon_end = pd.date_range(start_date, end_date, freq='M', closed='left')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:00:01.251777\n",
      "\n",
      "\n",
      "export new-york completed!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    frequency=3\n",
    "    start_date = '01-DEC-2018'\n",
    "    end_date = '30-DEC-2018'\n",
    "    api_key = '25c4af770ad84987be7181434232203'\n",
    "    location_list = ['new-york']\n",
    "\n",
    "    hist_weather_data = retrieve_hist_data(api_key,\n",
    "                                    location_list,\n",
    "                                    start_date,\n",
    "                                    end_date,\n",
    "                                    frequency,\n",
    "                                    location_label = False,\n",
    "                                    export_csv = True,\n",
    "                                    store_df = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
